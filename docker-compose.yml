services:
  fastapi:
    build:
      context: ./python-fastapi
      dockerfile: Dockerfile
    container_name: fastapi
    ports:
      - "8000:8000"
    environment:
      UPLOADS_DIR: "data/uploads"
      PROCESSED_DIR: "data/processed"
      OLLAMA_BASE_URL: "http://ollama:11434"
      OLLAMA_MODEL: "llama3.2:3b"
    depends_on:
      ollama:
        condition: service_started
      ollama-pull:
        condition: service_completed_successfully

  streamlit:
    build:
      context: ./python-streamlit
      dockerfile: Dockerfile
    container_name: streamlit
    ports:
      - "8501:8501"
    environment:
      BACKEND_URL: "http://fastapi:8000"
    depends_on:
      fastapi:
        condition: service_started

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "ollama list >/dev/null 2>&1"]
      interval: 5s
      timeout: 3s
      retries: 60

  ollama-pull:
    image: ollama/ollama:latest
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ollama:/root/.ollama
    environment:
      OLLAMA_HOST: "http://ollama:11434"
    entrypoint: ["/bin/sh", "-lc"]
    command: >
      "ollama pull llama3.2:3b"

volumes:
  ollama:
